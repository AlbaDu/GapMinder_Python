{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.4 64-bit ('base': conda)",
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "55d852cad6d72cd63413427ae4f5418f84914edf09935ce27cf78d516463fe5a"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# machine learning libraries\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ML_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.read_csv(r'..\\Gap_Minder\\Processed_data\\merged_data.csv')\n",
    "target = pd.read_csv(r'..\\Gap_Minder\\Processed_data\\target_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_for_train = ['population density', 'urban growth', 'population','population_growth', 'co2_emissions', 'life_expectancy']\n",
    "column_for_test = ['target']\n",
    "year = '1960'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-dtype, length and name of columns-\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10555 entries, 0 to 10554\n",
      "Data columns (total 8 columns):\n",
      "country               10555 non-null object\n",
      "year                  10555 non-null int64\n",
      "population density    10555 non-null float64\n",
      "urban growth          10555 non-null float64\n",
      "population            10555 non-null float64\n",
      "population_growth     10555 non-null float64\n",
      "co2_emissions         10555 non-null float64\n",
      "life_expectancy       10555 non-null float64\n",
      "dtypes: float64(6), int64(1), object(1)\n",
      "memory usage: 659.8+ KB\n",
      "None\n",
      "\n",
      "Index(['country', 'year', 'population density', 'urban growth', 'population',\n",
      "       'population_growth', 'co2_emissions', 'life_expectancy'],\n",
      "      dtype='object')\n",
      "\n",
      "(10555, 8)\n",
      "\n",
      "-Presence of NaNs in df-\n",
      "country               False\n",
      "year                  False\n",
      "population density    False\n",
      "urban growth          False\n",
      "population            False\n",
      "population_growth     False\n",
      "co2_emissions         False\n",
      "life_expectancy       False\n",
      "dtype: bool\n",
      "\n",
      "-dtype, length and name of columns-\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 169 entries, 0 to 168\n",
      "Data columns (total 57 columns):\n",
      "country    169 non-null object\n",
      "1960       25 non-null float64\n",
      "1961       25 non-null float64\n",
      "1962       25 non-null float64\n",
      "1963       25 non-null float64\n",
      "1964       25 non-null float64\n",
      "1965       26 non-null float64\n",
      "1966       26 non-null float64\n",
      "1967       26 non-null float64\n",
      "1968       26 non-null float64\n",
      "1969       26 non-null float64\n",
      "1970       26 non-null float64\n",
      "1971       109 non-null float64\n",
      "1972       109 non-null float64\n",
      "1973       109 non-null float64\n",
      "1974       109 non-null float64\n",
      "1975       109 non-null float64\n",
      "1976       109 non-null float64\n",
      "1977       109 non-null float64\n",
      "1978       109 non-null float64\n",
      "1979       109 non-null float64\n",
      "1980       109 non-null float64\n",
      "1981       110 non-null float64\n",
      "1982       110 non-null float64\n",
      "1983       110 non-null float64\n",
      "1984       110 non-null float64\n",
      "1985       111 non-null float64\n",
      "1986       111 non-null float64\n",
      "1987       111 non-null float64\n",
      "1988       111 non-null float64\n",
      "1989       111 non-null float64\n",
      "1990       158 non-null float64\n",
      "1991       132 non-null float64\n",
      "1992       132 non-null float64\n",
      "1993       132 non-null float64\n",
      "1994       132 non-null float64\n",
      "1995       134 non-null float64\n",
      "1996       134 non-null float64\n",
      "1997       134 non-null float64\n",
      "1998       134 non-null float64\n",
      "1999       134 non-null float64\n",
      "2000       136 non-null float64\n",
      "2001       136 non-null float64\n",
      "2002       136 non-null float64\n",
      "2003       136 non-null float64\n",
      "2004       166 non-null float64\n",
      "2005       167 non-null float64\n",
      "2006       167 non-null float64\n",
      "2007       167 non-null float64\n",
      "2008       137 non-null float64\n",
      "2009       137 non-null float64\n",
      "2010       137 non-null float64\n",
      "2011       137 non-null float64\n",
      "2012       137 non-null float64\n",
      "2013       137 non-null float64\n",
      "2014       131 non-null float64\n",
      "2015       34 non-null float64\n",
      "dtypes: float64(56), object(1)\n",
      "memory usage: 75.4+ KB\n",
      "None\n",
      "\n",
      "Index(['country', '1960', '1961', '1962', '1963', '1964', '1965', '1966',\n",
      "       '1967', '1968', '1969', '1970', '1971', '1972', '1973', '1974', '1975',\n",
      "       '1976', '1977', '1978', '1979', '1980', '1981', '1982', '1983', '1984',\n",
      "       '1985', '1986', '1987', '1988', '1989', '1990', '1991', '1992', '1993',\n",
      "       '1994', '1995', '1996', '1997', '1998', '1999', '2000', '2001', '2002',\n",
      "       '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011',\n",
      "       '2012', '2013', '2014', '2015'],\n",
      "      dtype='object')\n",
      "\n",
      "(169, 57)\n",
      "\n",
      "-Presence of NaNs in df-\n",
      "country    False\n",
      "1960        True\n",
      "1961        True\n",
      "1962        True\n",
      "1963        True\n",
      "1964        True\n",
      "1965        True\n",
      "1966        True\n",
      "1967        True\n",
      "1968        True\n",
      "1969        True\n",
      "1970        True\n",
      "1971        True\n",
      "1972        True\n",
      "1973        True\n",
      "1974        True\n",
      "1975        True\n",
      "1976        True\n",
      "1977        True\n",
      "1978        True\n",
      "1979        True\n",
      "1980        True\n",
      "1981        True\n",
      "1982        True\n",
      "1983        True\n",
      "1984        True\n",
      "1985        True\n",
      "1986        True\n",
      "1987        True\n",
      "1988        True\n",
      "1989        True\n",
      "1990        True\n",
      "1991        True\n",
      "1992        True\n",
      "1993        True\n",
      "1994        True\n",
      "1995        True\n",
      "1996        True\n",
      "1997        True\n",
      "1998        True\n",
      "1999        True\n",
      "2000        True\n",
      "2001        True\n",
      "2002        True\n",
      "2003        True\n",
      "2004        True\n",
      "2005        True\n",
      "2006        True\n",
      "2007        True\n",
      "2008        True\n",
      "2009        True\n",
      "2010        True\n",
      "2011        True\n",
      "2012        True\n",
      "2013        True\n",
      "2014        True\n",
      "2015        True\n",
      "dtype: bool\n",
      "\n",
      "-Number of NaNs in df-\n",
      "country      0\n",
      "1960       144\n",
      "1961       144\n",
      "1962       144\n",
      "1963       144\n",
      "1964       144\n",
      "1965       143\n",
      "1966       143\n",
      "1967       143\n",
      "1968       143\n",
      "1969       143\n",
      "1970       143\n",
      "1971        60\n",
      "1972        60\n",
      "1973        60\n",
      "1974        60\n",
      "1975        60\n",
      "1976        60\n",
      "1977        60\n",
      "1978        60\n",
      "1979        60\n",
      "1980        60\n",
      "1981        59\n",
      "1982        59\n",
      "1983        59\n",
      "1984        59\n",
      "1985        58\n",
      "1986        58\n",
      "1987        58\n",
      "1988        58\n",
      "1989        58\n",
      "1990        11\n",
      "1991        37\n",
      "1992        37\n",
      "1993        37\n",
      "1994        37\n",
      "1995        35\n",
      "1996        35\n",
      "1997        35\n",
      "1998        35\n",
      "1999        35\n",
      "2000        33\n",
      "2001        33\n",
      "2002        33\n",
      "2003        33\n",
      "2004         3\n",
      "2005         2\n",
      "2006         2\n",
      "2007         2\n",
      "2008        32\n",
      "2009        32\n",
      "2010        32\n",
      "2011        32\n",
      "2012        32\n",
      "2013        32\n",
      "2014        38\n",
      "2015       135\n",
      "dtype: int64\n",
      "                 country  year  population density  urban growth  population  \\\n",
      "0            Afghanistan  1960               13.80        0.0516   9000000.0   \n",
      "1                Albania  1960               59.70        0.0539   1640000.0   \n",
      "2                Algeria  1960                4.64        0.0553  11100000.0   \n",
      "3                 Angola  1960                4.38        0.0453   5450000.0   \n",
      "4    Antigua and Barbuda  1960              123.00        0.0338     54100.0   \n",
      "..                   ...   ...                 ...           ...         ...   \n",
      "163            Venezuela  1960                9.23        0.0592   8140000.0   \n",
      "164              Vietnam  1960              105.00        0.0532  32700000.0   \n",
      "165                Yemen  1960               10.10        0.0585   5320000.0   \n",
      "166               Zambia  1960                4.13        0.0741   3070000.0   \n",
      "167             Zimbabwe  1960                9.76        0.0498   3780000.0   \n",
      "\n",
      "     population_growth  co2_emissions  life_expectancy  \n",
      "0               0.0183          0.046             39.3  \n",
      "1               0.0302          1.240             62.2  \n",
      "2               0.0252          0.557             52.5  \n",
      "3               0.0137          0.101             40.6  \n",
      "4               0.0169          0.677             63.3  \n",
      "..                 ...            ...              ...  \n",
      "163             0.0365          7.010             60.7  \n",
      "164             0.0302          0.229             54.4  \n",
      "165             0.0141          0.684             33.0  \n",
      "166             0.0301          1.420             49.2  \n",
      "167             0.0330          1.570             53.2  \n",
      "\n",
      "[168 rows x 8 columns]\n",
      "                 country  target\n",
      "0                Albania     0.0\n",
      "1                Algeria     0.0\n",
      "2                 Angola     0.0\n",
      "3    Antigua and Barbuda     0.0\n",
      "4              Argentina     0.0\n",
      "..                   ...     ...\n",
      "164            Venezuela     0.0\n",
      "165              Vietnam     0.0\n",
      "166                Yemen     0.0\n",
      "167               Zambia     0.0\n",
      "168             Zimbabwe     0.0\n",
      "\n",
      "[169 rows x 2 columns]\n",
      "LinearRegression(copy_X=True, fit_intercept=False, n_jobs=None, normalize=False)\n",
      "\n",
      "- Confusion Matrix\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 1 0 0]]\n",
      "- Accuracy: 0.0\n",
      "\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=0, solver='warn', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "\n",
      "- Confusion Matrix\n",
      "[[46  0  0  0  0]\n",
      " [ 1  0  0  0  0]\n",
      " [ 1  0  0  0  0]\n",
      " [ 1  0  0  0  0]\n",
      " [ 1  0  0  0  0]]\n",
      "- Accuracy: 0.92\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=5, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=10,\n",
      "                       n_jobs=None, oob_score=False, random_state=17, verbose=0,\n",
      "                       warm_start=False)\n",
      "\n",
      "- Confusion Matrix\n",
      "[[45  0  0  0  1  0  0  0]\n",
      " [ 1  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  1  0  0  0  0  0]\n",
      " [ 1  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  0  0  0  0]]\n",
      "- Accuracy: 0.9\n",
      "\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
      "                       max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort=False,\n",
      "                       random_state=17, splitter='best')\n",
      "\n",
      "- Confusion Matrix\n",
      "[[43  0  1  2  0  0  0  0]\n",
      " [ 1  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  1  0  0  0]\n",
      " [ 1  0  0  0  0  0  0  0]\n",
      " [ 1  0  0  0  0  0  0  0]]\n",
      "- Accuracy: 0.86\n",
      "\n",
      "GradientBoostingClassifier(criterion='friedman_mse', init=None, learning_rate=1,\n",
      "                           loss='deviance', max_depth=2, max_features=2,\n",
      "                           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                           min_impurity_split=None, min_samples_leaf=1,\n",
      "                           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                           n_estimators=20, n_iter_no_change=None,\n",
      "                           presort='auto', random_state=0, subsample=1.0,\n",
      "                           tol=0.0001, validation_fraction=0.1, verbose=0,\n",
      "                           warm_start=False)\n",
      "\n",
      "- Confusion Matrix\n",
      "[[40  0  1  1  1  0  1  0  1  0  1  0  0]\n",
      " [ 1  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  1  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  1  0  0]\n",
      " [ 0  0  0  0  0  0  0  1  0  0  0  0  0]]\n",
      "- Accuracy: 0.8\n",
      "\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.5, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
      "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
      "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "\n",
      "- Confusion Matrix\n",
      "[[44  0  2  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  1  0  0  0]\n",
      " [ 1  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  1  0  0  0  0]]\n",
      "- Accuracy: 0.88\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ML_model = ML_models.main_sequence(merged, target, columns_for_train, column_for_test, year)\n",
    "ML_model"
   ]
  }
 ]
}